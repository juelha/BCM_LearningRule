<!DOCTYPE html>
<html>
<head>
<title>report.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="bcm-learning-rule">BCM Learning Rule</h1>
<p>Report for the final project of the course &quot;Neural Information Processing&quot; in SoSe 2025 by Julia Hattendorf.</p>
<h2 id="introduction">Introduction</h2>
<p>The BCM (Bienenstock, Cooper, and Munro) rule was introduced in 1982 to model the development of stimulus sensitivity in neurons. In the visual cortex, for example, neurons become selectively responsive to specific orientations or spatial features based on their sensory experience.
The rule proposes that a neuron's synaptic strengths are potentiated (LTP) or depressed (LTD) based on its average activity over time.</p>
<h2 id="the-model">The Model</h2>
<p>Note: Variable names from the original paper have been adjusted to modern machine learning conventions for readability.</p>
<p>The forward pass is the usual weighted sum:
$$
y = \sum_i w_i x_i
$$</p>
<p>The BCM rule differs to hebbian learning in that the update of the weight is not a product of the presynaptic and postsynaptic activity but of the presynaptic activity, $x$, and a modification function, $\phi$, of the postsynaptic activity, $y$, and a threshold $\theta$.
Additionally, the authors allow the option for a weight decay term $- \epsilon w_i$ with a variable of decay $\epsilon$.
For the $ith$ weight, the update looks like this:
$$
\frac{dw_i}{dt} = \phi(y) x_i -\epsilon w_i
$$</p>
<p>The modification function and its threshold, $\theta$, make sure that the weight adaption relates to the postsynaptic activity: If the activity of a neuron low $(y&lt;\theta)$ then its weight update will be negative. Vice versa, if it is higher $(y&gt;\theta)$ the weight update will be positive. The weights should thus adapt to favor some inputs over others, so the rule induces competition between inputs.</p>
<p>The threshold itself also adapts based on the postsynaptic activity. So if the activity rises or falls so will the threshold. Ideally, the threshold would then prevent runaway activation.</p>
<p>In the original paper the threshold is calculated with the temporal average value, $E(\cdot)$, of the activity $y$ and a positive constant $y_o$. The average value is additionally taken to the power of another positive constant $p$.</p>
<p>$$
\theta = E^p [(y/y_o)]
$$</p>
<p>Interestingly, after this threshold model was introduced, biological evidence for it could be found in experiments (Cooper &amp; Bear, 2012).</p>
<div style="page-break-after: always;"></div>
<h2 id="implementation--problems">Implementation &amp; Problems</h2>
<h3 id="dataset">Dataset</h3>
<p>For the dataset I chose synthetic oriented gratings as inputs. I then chose to construct an MNIST-like dataset, with 28x28 pixel per image and to repeat the orientations to have 400 samples in total. The dataset then had the shape (400, 784).</p>
<div align="center">
<img src="figs\gratings.png" width="450"/>
<figcaption>8 orientations evenly spaced over 180Â°.  </figcaption>
</a></p></div>
<h3 id="implementation-choices">Implementation Choices</h3>
<p>I implemented the BCM learning rule in a class with PyTorch tensors. This allows for easy access of hyperparameters and traces. As is convention, I also implemented the update to be calculated on data batches (SGD-like).</p>
<p>For the threshold, instead of a temporal average, I calculated the average of output activity per neuron over the input batch (using spatial instead of temporal average is discussed by Intrator &amp; Cooper, (1992)).</p>
<p>For the modification function some sources state that the original function used in the paper is $\phi = y (y-  \theta)$ (Udeigwe et al., 2017) (Blais &amp; Cooper, 2008). Although I could not confirm this with the paper, I chose it as a starting point.</p>
<h3 id="problem-runaway-activation">Problem: Runaway Activation</h3>
<p>The BCM rule is supposed to solve runaway activity in hebbian learning but when I started implementing this rule, I had a big problem: runaway activation.
When using the function $\phi = y (y-  \theta)$ it is quite apparent that there is no upper bound to modifications and one can only rely on the threshold.</p>
<p>Running the model for 1 epoch, with 10 neurons and a batch size of 100:</p>
<div align="center">
<img src="figs\runawayactivation.png" width="350"/>
<figcaption>The activity of a single neuron while training.  </figcaption>
</a></p></div>
<div align="center">
<img src="figs\single_neuron.png" width="450"/>
<figcaption>The weights of all neurons after training.  </figcaption>
</a></p></div>
<p>Notice that one neuron is dominating. In some runs, weights of other neurons are growing too but generally the weights of one neuron dominate. My interpretation of this is that the neuron forms weights that are good enough to be a &quot;catch-all&quot; for many different inputs. For example, the weights of neuron 8 above in the figure could activate for vertical, horizontal, and diagonal input patterns. Having a catch-all weight matrix means the neuron's activity will always be above the threshold, further adjusting the weights positively. Learning for this neuron will result in a &quot;snowball effect&quot;.</p>
<h4 id="solution-1-weight-normalization">Solution 1: Weight normalization</h4>
<p>The simplest fix for too large weights seems to be to simply normalize the weight in every update.</p>
<p>While that does help with the runaway activation problem, we still have the &quot;snowball&quot; problem of one neuron dominating:</p>
<div align="center">
<img src="figs\runawayactivation_norm.png" width="350"/>
<figcaption>The activity of a single neuron while training.  </figcaption>
</a></p></div>
<div align="center">
<img src="figs\single_neuron_norm.png" width="450"/>
<figcaption>The weights of all neurons after training.  </figcaption>
</a></p></div>
<h4 id="solution-2-phi-rightarrow-tanh">Solution 2: $\phi \rightarrow tanh$</h4>
<p>Since some sources show the modification function with a sigmoid-ish trajectory for $y&gt;\theta$, I tried using a $tanh$-based function to model both the LTD and LTP effect.</p>
<div align="center">
<img src="figs\phi_book.png" width="350"/> <img src="figs\tanh_mod.png" width="250"/>
</a></p></div>
<p>On the left the modification function according to the book Neuronal Dynamics by Gerstner et al. (2014). On the right my own variant $phi = tanh(2 * (y - \theta))$.</p>
<p>This also helped with the runaway problem but did not give promising results in terms of feature extraction.</p>
<div align="center">
<img src="figs\tanh_y_t.png" width="350"/>
<figcaption>The activity and threshold of a single neuron while training.  </figcaption>
</a></p></div>
<div align="center">
<img src="figs\tanh_w.png" width="450"/>
<figcaption>The weights of all neurons after training.  </figcaption>
</a></p></div>
<h4 id="solution-3-change-the-implementation-of-bcm">Solution 3: Change the implementation of BCM</h4>
<p>In order to find another solution, I looked at the implementation for the paper (Squadrani et al., 2022) that can be found on the <a href="!https://github.com/Nico-Curti/plasticity/blob/main/plasticity/model/bcm.py">github account of Nico Curti</a>. They use the implementation proposed by Law and Cooper in 1994.
The differences to the original BCM implementation are:</p>
<ul>
<li>adding ReLU as activation function: $y = ReLU(\sum_i w_i x_i)$</li>
<li>calculating the threshold with the moving average: $\theta_t = \gamma \theta_{t-1} + (1-\gamma) \langle y^2 \rangle_{b_t}$ where $\langle \cdot \rangle_{b_t}$ is the average over the batch of training patterns</li>
<li>dividing the weight update by the threshold: $\frac{dw_i}{dt} = \frac{y (y-  \theta) x_i}{\theta}$</li>
</ul>
<p>I found that no single of those differences is enough to ensure stability, but that all three are needed. After implementing all these changes, the form of the weight is improved and the orientations are picked up:</p>
<div align="center">
<img src="figs\curti_og.png" width="450"/>
<figcaption>The weights of all neurons after training.  </figcaption>
</a></p></div>
<p>Additionally, the relationship of threshold and activity is clearer. Below one can clearly see how spikes in output activity are picked up by the threshold and how the activity is regulated down:</p>
<div align="center">
<img src="figs\mod_curti.png" width="350"/>
<figcaption>The activity and threshold of a single neuron while training.  </figcaption>
</a></p></div>
<h3 id="timescales">Timescales</h3>
<p>To add another layer of complexity to this implementation, I added separate timescales for the change of threshold, $\tau_\theta$, and the weight adaption, $\tau_w$. To find an intuition for good values, I consulted the paper by Udeigwe et al. (2017), where the authors examined the dynamical properties for different values of $\tau_\theta$ and $\tau_w$. From the paper I gathered that $\tau_\theta$ &gt; $\tau_w$.</p>
<p>I unfortunately could not set up a proper grid-search for these parameters due to time constraints and manually tried out a few combinations:</p>
<div align="center">
<img src="figs\curti_config1.png" width="450"/>
<figcaption>Weights for tau_th = 0.1 and
tau_w = 0.05  </figcaption>
</a></p></div>
<div align="center">
<img src="figs\curti_config2.png" width="450"/>
<figcaption>Weights for tau_th = 0.1 and
tau_w = 0.001 </figcaption>
</a></p></div>
<p>The last combination of $\tau_\theta = 0.1$ and $\tau_w = 0.001$ is the one I will be using from now on for the analysis.</p>
<h2 id="results-and-analysis">Results and Analysis</h2>
<h3 id="threshold-preventing-runaway-activity">Threshold preventing runaway activity</h3>
<p>The threshold is successfully regulating the activity. Looking at t=20 and t=25, the spike in activation is picked up by the threshold and the activation in the next step is reduced.</p>
<div align="center">
<img src="figs\mod_working.png" width="350"/>
<figcaption>The activity and threshold of a single neuron while training. </figcaption>
</a></p></div>
<h3 id="selectivity-increases-with-learning">Selectivity increases with learning</h3>
<p>To have some measure for the &quot;performance&quot; of a neuron, I chose selectivity, i.e. how a neuron responds to certain stimuli compared to others.
The general index for selectivity as proposed by Bienenstock, Cooper and Munro (Eq. 3, p. 33):
$$
Sel_d(N) = 1 - \frac{\text{mean response of N  with respect to d} }{\text{max response of N with respect to d}}
$$</p>
<div align="center">
<img src="figs\sel.png" width="250"/>
<figcaption>The selectivity of all neurons is increasing over training. </figcaption>
</a></p></div>
<h3 id="stability">Stability</h3>
<p>When looking at the activity of the neurons for 5 epochs (left) it looks like the runaway problem might still be there. However, training the neurons for 50 epochs (right) showed that the activity eventually converges. Both were trained with a batch size of 100.</p>
<div align="center">
    <img src="figs\stab_1.png"  width="250" >  <img src="figs\stab_2.png"  width="250"> 
    <figcaption>The activity of the neurons converge. Left trained for 5 epochs, right for 50 epochs.  </figcaption>
</a></p></div>
<p>Additionally, I ran this model on MNIST to see if it can still perform well for a different dataset:</p>
<div align="center">
<img src="figs\MNIST.png" width="350"/>
<figcaption>Weights for MNIST.  </figcaption>
</a></p></div>
<h2 id="comparison">Comparison</h2>
<p>For comparison purposes, the learning rate is $\eta$ = 0.05 for all models, which have been trained for 5 epochs with a batch size of 100.</p>
<h3 id="hebb-with-weight-normalization">Hebb with weight normalization</h3>
<p>Training the neurons with the (normed) hebb rule seems to lead to the same input image being picked up across all neurons. In different runs, different images are dominant:</p>
<div align="center">
<img src="figs\comp_hebb_1.png" width="450"/>
<figcaption>The weights of all neurons after training with hebb.  </figcaption>
</a></p></div>
<div align="center">
<img src="figs\comp_hebb_2.png" width="450"/>
<figcaption>The weights of all neurons after training with hebb.  </figcaption>
</a></p></div>
<h3 id="oja">Oja</h3>
<p>With Oja's rule I needed to train the network a little longer to achieve similar results to hebb and BCM.</p>
<p>For comparison, here is Oja's rule with 5 epochs:</p>
<div align="center">
<img src="figs\comp_oja_1.png" width="450"/>
<figcaption>The weights of all neurons after training with Oja's rule.  </figcaption>
</a></p></div>
<p>And here for 10 epochs:</p>
<div align="center">
<img src="figs\comp_oja_2.png" width="450"/>
<figcaption>The weights of all neurons after training with Oja's rule. </figcaption>
</a></p></div>
<div style="page-break-after: always;"></div>
<h2 id="future-ideas">Future ideas</h2>
<h4 id="wta--bcm">WTA &amp; BCM</h4>
<p>Since BCM induces competition on an input pattern level and WTA induces competition on a neuron level, it would be interesting to try and merge them together and see what happens. However, it is noted that both models come with a potential &quot;snowball effect&quot; problem of one neuron being chosen over and over, either due to winning the competition or due to having a catch-all matrix and having high activations.</p>
<p>I attempted to combine the two, but am not quite satisfied with the results. Below are the weights trained with this combination and $\tau_\theta$ = 1 and $\tau_w$ = 0.5.</p>
<div align="center">
<img src="figs\BCMWTA_th1_w05.png" width="450"/>
<figcaption>The weights of all neuros after training with a WTA & BCM combination.  </figcaption>
</a></p></div>
<h4 id="investigating-timescales">Investigating Timescales</h4>
<p>My implementation has been based on the assumption that the &quot;homeostatic&quot; timescale, i.e. $\tau_\theta$ is faster than the &quot;learning&quot; time scale, i.e. $\tau_w$.
I gathered this assumption from the work of Udeigwe et al. (2017).
However, the authors also mention that this finding seems to be somewhat contradictory to the findings gathered from biological experiments, where the process of homeostasis seems to be slower than the process of learning.
The authors also mention that there may be slow and fast homeostatic processes.
It would be interesting to investigate this dilemma further.</p>
<h4 id="effect-of-inhibitory-synapses">Effect of Inhibitory Synapses</h4>
<p>In the original paper, the authors mentioned that one could show the effect of &quot;inhibition&quot; by setting the negative values of the weight update to zero. It would be interesting to see how the learning is affected by this and to try &quot;weighting&quot; the LTD- and the LTP-part differently.</p>
<h4 id="activation-functions">Activation functions</h4>
<p>The BCM model does not provide an activation function for the postsynaptic activity. However, using an activation function was useful for me to achieve stable results. Trying out different activation functions and how they affect results would be interesting.</p>
<div style="page-break-after: always;"></div>
<h2 id="literature">Literature</h2>
<p>Bienenstock, E. L., Cooper, L. N., &amp; Munro, P. W. (1982). Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex. Journal of Neuroscience, 2(1), 32-48. <br>
accessed via https://www.jneurosci.org/content/jneuro/2/1/32.full.pdf</p>
<p>Blais, B. S., &amp; Cooper, L. (2008). BCM theory. Scholarpedia, 3(3), 1570.<br>
accessed via http://www.scholarpedia.org/article/BCM_theory</p>
<p>Cooper, L. N., &amp; Bear, M. F. (2012). The BCM theory of synapse modification at 30: interaction of theory with experiment. Nature Reviews Neuroscience, 13(11), 798-810. <br>
accessed via https://brabeeba.github.io/neuralReadingGroup/cooper.pdf</p>
<p>Gerstner, W., Kistler, W. M., Naud, R., &amp; Paninski, L. (2014). Neuronal dynamics: From single neurons to networks and models of cognition. Cambridge University Press.<br>
accessed via https://neuronaldynamics.epfl.ch/online/Ch19.S2.html</p>
<p>Intrator, N., &amp; Cooper, L. N. (1992). Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. Neural Networks, 5(1), 3-17.<br>
accessed via https://www.sciencedirect.com/science/article/pii/S0893608005800036</p>
<p>Squadrani, L., Curti, N., Giampieri, E., Remondini, D., Blais, B., &amp; Castellani, G. (2022). Effectiveness of Biologically Inspired Neural Network Models in Learning and Patterns Memorization. Entropy, 24(5), 682. <br>
accessed via https://pmc.ncbi.nlm.nih.gov/articles/PMC9141587/  <br>
Implementation on Nico Curti's github: https://github.com/Nico-Curti/plasticity/blob/main/plasticity/model/bcm.py</p>
<p>Udeigwe, L. C., Munro, P. W., &amp; Ermentrout, G. B. (2017). Emergent dynamical properties of the BCM learning rule. The Journal of Mathematical Neuroscience, 7, 1-32.<br>
accessed via https://mathematical-neuroscience.springeropen.com/articles/10.1186/s13408-017-0044-6</p>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });</script>

</body>
</html>
