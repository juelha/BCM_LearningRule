{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff08456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import numpy as np\n",
    "#import utils \n",
    "from utils import draw_weights, selectivity_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HebbNet:\n",
    "\n",
    "    def __init__(self, input_dim: int, n_units: int = 64, rule: str = \"oja\", norm_weights=False, mode=\"normal\"):\n",
    "        assert rule in (\"oja\", \"hebb\", \"BCM\"), \"rule must be 'oja' or 'hebb' or 'BCM' \" \n",
    "        assert mode in (\"normal\", \"WTA\"), \"rule must be 'normal' or 'WTA'\"\n",
    "        self.rule = rule\n",
    "        self.mode = mode\n",
    "        self.norm_weights = norm_weights\n",
    "        self.rng  = np.random.default_rng()\n",
    "        self.input_dim = input_dim\n",
    "        self.W    = self.rng.normal(0, 0.1, size=(n_units, input_dim)).astype(np.float32)\n",
    "        self.W   /= np.linalg.norm(self.W, axis=1, keepdims=True) + 1e-9\n",
    "        self.threshold = np.zeros(n_units) # self.rng.normal(0, 0.1, size=(n_units)).astype(np.float32)\n",
    "\n",
    "        # BCM hps\n",
    "        self.gamma = 0.5\n",
    "        self.tau_th= 0.09\n",
    "        self.tau_w = 0.001\n",
    "\n",
    "    # Your job: Forward + learning\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        # implement the forward pass for the hebbain activations.\n",
    "\n",
    "        return X @ self.W.T\n",
    "\n",
    "    def update(self, X: np.ndarray, eta: float):\n",
    "        # implement the hebbian learning rules (and with winner take all). Do this for a whole batch at a time!\n",
    "        # you can switch between different rules based on self.rule\n",
    "        Y = self.forward(X)        \n",
    "\n",
    "        if self.mode == \"normal\":\n",
    "            # iterate over batch\n",
    "            for i in range(batch_size):\n",
    "                    \n",
    "                # pick corresponding input and output vectors, otherwise dims when x * y wont work out \n",
    "                x_vec = X[i,:] \n",
    "                y_vec = Y[i,:]\n",
    "\n",
    "                if self.rule == \"hebb\":\n",
    "                     self.W += eta * np.outer(y_vec, x_vec)\n",
    "\n",
    "                if self.rule == \"oja\":          \n",
    "                    y_vec = y_vec.reshape(-1)               \n",
    "                    delta_w = np.outer(y_vec , ( x_vec - np.dot(y_vec, self.W)))\n",
    "                    self.W += eta * delta_w\n",
    "\n",
    "            self.W /= batch_size # after iterating over batch div with batchsize \n",
    "\n",
    "            if self.norm_weights:\n",
    "                self.W /= np.linalg.norm(self.W) + 1e-9\n",
    "\n",
    "            \n",
    "        if self.mode == \"WTA\": \n",
    "            # pick winners per input vec (img)\n",
    "            winners = Y.argmax(axis=1)\n",
    "            for i, w in enumerate(winners):\n",
    "                x_vec = X[i]    # pick input that the neuron \"won\"\n",
    "                y = Y[i, w]     # pick output (scalar) of winning neuron    \n",
    "\n",
    "                if self.rule == \"hebb\": \n",
    "                        self.W[w] += eta * y * x_vec \n",
    "\n",
    "                if self.rule == \"oja\":\n",
    "                    delta_w = y * ( x_vec - y * self.W[w])\n",
    "                    self.W[w] += eta * delta_w\n",
    "\n",
    "                if self.rule == \"BCM\":\n",
    "                    # calc current threshold \n",
    "                    cur_threshold =  np.mean(Y**2, axis=0)[w] # avg of neuron over batch \n",
    "                    print(cur_threshold)\n",
    "\n",
    "                    # update threshold using moving avg \n",
    "                    self.threshold[w] = ( self.gamma*self.threshold[w] + (1-self.gamma) * cur_threshold ) * self.tau_th\n",
    "\n",
    "                    # calc delta W\n",
    "                    t  = cur_threshold #self.threshold[w]\n",
    "                    delta_w = ( (y * ( y - t ) )  * x_vec) / t\n",
    "                    delta_w /= X.shape[0] # normalize the weights update according to the number of samples\n",
    "\n",
    "                    # update weights\n",
    "                    self.W[w] += delta_w * self.tau_w  \n",
    "                    #print(\"w\", self.weights)\n",
    "\n",
    "                if self.norm_weights:\n",
    "                    self.W[w] /= np.linalg.norm(self.W[w]) + 1e-9\n",
    "\n",
    "    # ----- helpers ------\n",
    "    \n",
    "    # Supervised linear read‑out (ridge regression)\n",
    "    def train_linear_classifier(self, X: np.ndarray, y: np.ndarray, reg: float = 1e-3):\n",
    "        \"\"\"Fit `W_out` via linear regression. This is technically a \"delta rule\".\n",
    "        \"\"\"\n",
    "        A = self.forward(X)                 # (N, H)\n",
    "        N, H = A.shape\n",
    "        classes = int(y.max()) + 1\n",
    "        # One‑hot encode labels → Y (N, C)\n",
    "        Y = np.zeros((N, classes), dtype=np.float32)\n",
    "        Y[np.arange(N), y] = 1.0\n",
    "        # Closed‑form ridge solution\n",
    "        I = np.eye(H, dtype=np.float32)\n",
    "        self.W_out = np.linalg.solve(A.T @ A + reg * I, A.T @ Y)  # (H, C), safe in the object\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Return predicted class indices using the trained linear read‑out.\"\"\"\n",
    "        if self.W_out is None:\n",
    "            raise RuntimeError(\"Linear classifier not trained. Call train_linear_classifier first.\")\n",
    "        logits = self.forward(X) @ self.W_out  # (B, C)\n",
    "        return logits.argmax(axis=1)\n",
    "\n",
    "    def linear_accuracy(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"Accuracy using the supervised read‑out (must be trained).\"\"\"\n",
    "        preds = self.predict(X)\n",
    "        return float((preds == y).mean())\n",
    "\n",
    "    # Evaluation with majority votes.\n",
    "    def majority_labels(self, X: np.ndarray, y: np.ndarray) -> list[int]:\n",
    "        winners = self.forward(X).argmax(axis=1)\n",
    "        bag = [[] for _ in range(self.W.shape[0])]\n",
    "        for w, lbl in zip(winners, y):\n",
    "            bag[w].append(lbl)\n",
    "        return [max(set(b), key=b.count) if b else -1 for b in bag]\n",
    "\n",
    "    def accuracy(self, X: np.ndarray, y: np.ndarray, unit_labels: Sequence[int]) -> float:\n",
    "        preds = np.take(unit_labels, self.forward(X).argmax(axis=1))\n",
    "        return float((preds == y).mean())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c942ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load data\n",
    "# data_np = np.loadtxt(\"gratings.csv\", delimiter=\",\")\n",
    "# data = torch.from_numpy(data_np)\n",
    "# n_samples, n_in = data.shape\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "data, y = fetch_openml(name='mnist_784', version=1, data_id=None, return_X_y=True)\n",
    "data = torch.tensor(data.to_numpy(), dtype=torch.float64)\n",
    "data /= 255.\n",
    "data -= torch.mean(data, dim=0) # balance data around mean \n",
    "# for mnist eps=0.001 !\n",
    "\n",
    "\n",
    "# preprocess\n",
    "\n",
    "\n",
    "print(data.shape)\n",
    "n_samples, n_in = data.shape\n",
    "\n",
    "# hps #\n",
    "n_epochs = 5\n",
    "n_units = 10\n",
    "batch_size = 100\n",
    "tau_th = 0.05\n",
    "tau_w = 0.001\n",
    "\n",
    "model = BCM_Model_Curti(n_in, n_units)\n",
    "\n",
    "# learning loop\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    data = data[torch.randperm(n_samples)]  # Shuffle the input data\n",
    "\n",
    "    # Iterate over all minibatches\n",
    "    for i in range(n_samples // batch_size):\n",
    "        minibatch = data[i * batch_size:(i + 1) * batch_size].T # transform to shape (n_in, batch_size)\n",
    "        model.update(minibatch, tau_th=tau_th, tau_w=tau_w)\n",
    "    draw_weights(model.weights, epoch)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CHU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
